# Quick Recipes ðŸ³

> **Goal:** Run an agent, call it, stream results â€“ all in less than tenÂ minutes.

---

## 1Â Â Hello Echo

Start the builtâ€‘in echo agent:

```bash
go run ./examples/basic-agent
# âžœ  Listening on :8080
```

Send a task (JSONâ€‘RPC):

```bash
curl -s -X POST localhost:8080/rpc \
  -d '{"jsonrpc":"2.0","id":1,"method":"tasks/send","params":{"id":"t1","message":{"role":"user","parts":[{"type":"text","text":"Ping"}]}}}' | jq .artifacts[0].parts[0].text

# "Ping"
```

ðŸŽ‰Â Congratulations â€“ you just used the A2A protocol.

---

## 2Â Â Listing Prompts

```bash
curl -s -X POST localhost:8080/rpc -d '{"jsonrpc":"2.0","id":2,"method":"prompts/list"}' | jq .prompts
```

Fetch a promptâ€™s full content:

```bash
curl -s -X POST localhost:8080/rpc -d '{"jsonrpc":"2.0","id":3,"method":"prompts/get","params":{"name":"Greeting"}}' | jq .messages[0].content.text
```

---

## 3Â Â Streaming with SSE

The SSE endpoint lives at `/events`.

```bash
# in a second terminal
curl -sN localhost:8080/events | jq -c
```

Back in the first terminal send a streaming request

```bash
curl -s -X POST localhost:8080/rpc \
  -d '{"jsonrpc":"2.0","id":4,"method":"sampling/createMessageStream","params":{"systemPrompt":"You are a poet.","messages":[]}}'
```

Tokens will appear live in the SSE stream.

---

## 4Â Â Next Steps

* Expose a file via the Resource manager (`resources/list`).
* Export `OPENAI_API_KEY` to enable real LLM completions.
* Continue with the deepâ€‘dives:
  * [Prompt Kitchen](prompts.md)
  * [Resource Pantry](resources.md)
  * [Sampling Lab](sampling.md)

Happy experiments!Â ðŸŽˆ
